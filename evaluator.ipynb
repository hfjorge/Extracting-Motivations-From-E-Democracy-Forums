{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aae1fab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.graphs import Neo4jGraph\n",
    "import os\n",
    "from langchain_ollama import ChatOllama\n",
    "import json\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01b834aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jocke\\AppData\\Local\\Temp\\ipykernel_61840\\278928253.py:2: LangChainDeprecationWarning: The class `Neo4jGraph` was deprecated in LangChain 0.3.8 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-neo4j package and should be used instead. To use it run `pip install -U :class:`~langchain-neo4j` and import as `from :class:`~langchain_neo4j import Neo4jGraph``.\n",
      "  graph = Neo4jGraph()\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "graph = Neo4jGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aabc45c",
   "metadata": {},
   "source": [
    "# Collect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1929722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nodes_by_topic_title(topic_title: str):\n",
    "    \"\"\"\n",
    "    Returns nodes from the Neo4j database where the 'topic_title' property matches the given value.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "       MATCH (p:Post)-[]-(a:Argument)-[]-(mn:MaxNeefCategory)\n",
    "       WHERE p.topic_title = $topic_title\n",
    "       RETURN p.id AS post_id,\n",
    "       a.description AS descriptions,\n",
    "       a.motivations_descriptions AS motivations,\n",
    "       collect(DISTINCT mn.id) AS categories\n",
    "       \"\"\"\n",
    "    \n",
    "    results = graph.query(query, params={\"topic_title\": topic_title})\n",
    "    return results #[record['n'] for record in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d96eff79",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3236cc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = get_nodes_by_topic_title(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b91b5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "{'post_id': 'test', 'descriptions': 'The design and architecture are sleek and modern', 'motivations': ['Appreciation for modern design and architecture due to their aesthetic appeal.'], 'categories': ['Creativity']}\n",
      "{'post_id': 'test', 'descriptions': \"It will be a fantastic addition to our city's infrastructure.\", 'motivations': ['Wants to improve the quality of life in the city.'], 'categories': ['Participation']}\n",
      "{'post_id': 'test', 'descriptions': 'The environmental impact assessments have clearly had a significant input in minimizing its footprint, which is great news for us locals who care about preserving our natural habitats.', 'motivations': ['Value for environmental protection and the role of EIAs in minimizing ecological impact, which benefits local communities who care about preserving nature.'], 'categories': ['Protection', 'Participation']}\n",
      "{'post_id': 'test', 'descriptions': \"It's clear that they're committed to making this airport a success for both the city and the local community.\", 'motivations': [\"Wants to ensure the airport's success benefits the local community.\"], 'categories': ['Affection']}\n",
      "{'post_id': 'test', 'descriptions': \"They've been doing a great job of keeping us informed through regular updates and open forums.\", 'motivations': ['Desire for effective information dissemination to ensure individuals are well-informed.', 'Interest in fostering community engagement through communication channels like forums.'], 'categories': ['Participation', 'Understanding']}\n",
      "{'post_id': 'test', 'descriptions': 'As someone who regularly commutes, it would be fantastic to know what options will be available for those without cars.', 'motivations': ['Desire to know about alternative transportation options for those without cars.'], 'categories': ['Freedom', 'Understanding']}\n",
      "{'post_id': 'test', 'descriptions': \"I'm excited to see this project come to fruition and think it will have a positive impact on our city's economy and residents.\", 'motivations': [\"Excitement about the project's success leading to positive economic and social impacts.\"], 'categories': ['Affection', 'Participation']}\n",
      "{'post_id': 'test', 'descriptions': \"One thing I'd love to see in the future is more details about the public transportation links and amenities that will be available to passengers.\", 'motivations': ['Wants more information about public transportation amenities to better understand what will be available.'], 'categories': ['Understanding']}\n"
     ]
    }
   ],
   "source": [
    "print(\"Results:\")\n",
    "for record in results:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5087d6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'post_id': 'msw2stv', 'descriptions': 'The President should have a clear and self-regulating mandate', 'motivations': ['Wants to ensure executive function is effective', 'Desires a system that prevents gerrymandering'], 'categories': ['Protection', 'Participation']}\n"
     ]
    }
   ],
   "source": [
    "post = next((item for item in results if item['post_id'] == \"msw2stv\"), None)\n",
    "print(post)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08793316",
   "metadata": {},
   "source": [
    "# Retrieve context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22645b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = ChatOllama(model=\"llama3.1\")\n",
    "\n",
    "resumos_cache = {} # To store summaries of previous posts\n",
    "\n",
    "# Estimate tokens based on word count\n",
    "def estimar_tokens(texto: str) -> int:\n",
    "    return int(len(texto.split()) * 1.3)  # reasonable approximation\n",
    "\n",
    "# Function to fetch the parents of a post (up to 3 levels)\n",
    "def get_parent_posts(post_id: str, max_levels: int = 3):\n",
    "    query = f\"\"\"\n",
    "    MATCH (child:Post {{id: $post_id}})-[:RESPONDS_TO*1..{max_levels}]->(parent)\n",
    "    WHERE (parent:Post OR parent:OriginalPost) AND toLower(parent.text) <> '[removed]'\n",
    "    RETURN parent.text AS text, parent.id AS id\n",
    "    ORDER BY size(parent.text) DESC\n",
    "    \"\"\"\n",
    "    results = graph.query(query, params={\"post_id\": post_id})\n",
    "    return [{\"id\": row[\"id\"], \"text\": row[\"text\"]} for row in results]\n",
    "\n",
    "# Function to fetch the text of the current post\n",
    "def get_post_text(post_id: str):\n",
    "    query = \"\"\"\n",
    "    MATCH (p:Post {id: $post_id})\n",
    "    RETURN p.text AS text\n",
    "    \"\"\"\n",
    "    result = graph.query(query, params={\"post_id\": post_id})\n",
    "    if result:\n",
    "        return result[0][\"text\"]\n",
    "    return None\n",
    "\n",
    "# Function to summarize text with LLM, checking size and using cache\n",
    "def resumir_texto(texto: str, token_threshold: int = 150) -> str:\n",
    "    if texto in resumos_cache:\n",
    "        return resumos_cache[texto]\n",
    "    \n",
    "    if estimar_tokens(texto) <= token_threshold:\n",
    "        resumo = texto.strip()\n",
    "    else:\n",
    "        prompt = f\"\"\"\n",
    "        Summarize the following post in a concise and informative way. If you refer to the author, make sure you refer to them as 'an author of previous posts'. Only keep what is essential to understand the point made, and return only the summary:\n",
    "\n",
    "        \\\"\\\"\\\"{texto}\\\"\\\"\\\"\n",
    "        \"\"\"\n",
    "        resumo = summarizer.invoke(prompt).content.strip()\n",
    "    \n",
    "    resumos_cache[texto] = resumo\n",
    "    return resumo\n",
    "\n",
    "# Main function to build summarized context from previous posts\n",
    "def get_context(post_id: str, max_pais: int = 3, token_threshold: int = 200):\n",
    "    pais = get_parent_posts(post_id, max_levels=max_pais)\n",
    "    post_filho = get_post_text(post_id)\n",
    "\n",
    "    if not post_filho:\n",
    "        raise ValueError(\"Child post not found.\")\n",
    "\n",
    "    contexto_resumido = []\n",
    "    for p in pais[:max_pais]:\n",
    "        resumo = resumir_texto(p[\"text\"], token_threshold=token_threshold)\n",
    "        contexto_resumido.append(f\"*Context from previous user's comment:* {resumo}\")\n",
    "\n",
    "    contexto_final = \"\\n\".join(contexto_resumido)\n",
    "    return contexto_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e97459",
   "metadata": {},
   "source": [
    "# Instantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cc7991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATIONS (you can adjust this as preferred)\n",
    "USE_OPENAI = False  # True for OpenAI, False for Ollama\n",
    "OPENAI_MODEL = \"gpt-4o\"\n",
    "OLLAMA_MODEL = \"deepseek-r1:8b\"\n",
    "\n",
    "# OpenAI Client (only if used)\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\") or \"INSERT_YOUR_KEY_HERE\"\n",
    "openai_client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "ollama_llm = ChatOllama(model=OLLAMA_MODEL, temperature=0, format=\"json\")\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are an evaluator specialized in assessing the credibility of automatic argument extraction from discussion posts.\n",
    "\n",
    "Below is a post and the extracted information by a language model:\n",
    "\n",
    "Original Post:\n",
    "\"{post_text}\"\n",
    "\n",
    "Extracted:\n",
    "- Argument: {argument}\n",
    "- Motivations: {motivation}\n",
    "- Max-Neef Categories: {categories}\n",
    "\n",
    "\n",
    "Max-Neef Categories refer to fundamental human needs based on Manfred Max-Neef's Human Scale Development theory. Each motivation should reflect one or more of these needs:\n",
    "\n",
    "1. **Subsistence** - physical health, food, shelter.\n",
    "2. **Protection** - safety, care, social security.\n",
    "3. **Affection** - relationships, love, friendship.\n",
    "4. **Understanding** - curiosity, education, knowledge.\n",
    "5. **Participation** - involvement, responsibility, belonging.\n",
    "6. **Leisure** - rest, fun, play, recreation.\n",
    "7. **Creativity** - innovation, self-expression, skills.\n",
    "8. **Identity** - sense of self, belonging, cultural roots.\n",
    "9. **Freedom** - autonomy, choice, equality.\n",
    "\n",
    "The arguments are to be simple bullet points, and the motivations are to be simple phrases associated with the arguments. The Max-Neef Categories are a list of categories categorizing the motivations with Max-Neef's theory. **Not every category needs to be present in each case**.\n",
    "\n",
    "Evaluate the following criteria:\n",
    "1. Is the argument extraction coherent with the post, reflecting the author's opinion?\n",
    "2. Do the motivations seem like a plausible justification for why the author made the argument?\n",
    "3. According to Max-Neef's Human Scale Development theory, do the categories accurately reflect the necessities associated with the defined motivations and arguments? How many categories are correctly identified?\n",
    "\n",
    "Assign a score for each criterion, and write brief feedback.\n",
    "\n",
    "Respond in the following JSON format:\n",
    "{{\n",
    "  \"argument_extraction\": 0-5, (where 0 means the argument is not coherent with the post, and 5 means it is very coherent)\n",
    "  \"motivation_plausibility\": 0-5, (where 0 means the motivations are not plausible, and 5 means they are very plausible)\n",
    "  \"category_score\": 0 - 100, (where 0 means you consider none of the categories are correct, and 100 means you consider all categories correct)\n",
    "  \"feedback\": \"Brief comment on strengths or weaknesses of the extraction. Ideal Max-Neef categorization.\"\n",
    "}}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def generate_evaluation_response(post_text, argument, motivation, categories, use_openai=USE_OPENAI):\n",
    "\n",
    "    prompt = prompt_template.format(\n",
    "        post_text=post_text.strip(),\n",
    "        argument=argument.strip(),\n",
    "        motivation=motivation,\n",
    "        categories=categories\n",
    "    )    \n",
    "        \n",
    "    if use_openai:\n",
    "        print(f\"ðŸ”— Using OpenAI ({OPENAI_MODEL})...\")\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=OPENAI_MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a critical evaluator of argument and motivation extraction.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    else:\n",
    "        print(f\"ðŸ’» Using local model via Ollama ({OLLAMA_MODEL})...\")       \n",
    "        response = ollama_llm.invoke(prompt)\n",
    "            \n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c723aa",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4795a74a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id = results[0].get(\"post_id\")\n",
    "id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2ad9298",
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a441043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*Comment to analyze:* I just wanted to share my thoughts on the new airport development in our area. As someone who has been following\n",
      "the progress closely, I'm thoroughly impressed with how the project is coming along.\n",
      "\n",
      "The design and architecture are sleek and modern, and I think it will be a fantastic addition to our city's\n",
      "infrastructure. The environmental impact assessments have clearly had a significant input in minimizing its\n",
      "footprint, which is great news for us locals who care about preserving our natural habitats.\n",
      "\n",
      "I'm also impressed with the transparency of the project managers - they've been doing a great job of keeping us\n",
      "informed through regular updates and open forums. It's clear that they're committed to making this airport a\n",
      "success for both the city and the local community.\n",
      "\n",
      "One thing I'd love to see in the future is more details about the public transportation links and amenities that\n",
      "will be available to passengers. As someone who regularly commutes, it would be fantastic to know what options\n",
      "will be available for those without cars.\n",
      "\n",
      "Overall, though, I'm excited to see this project come to fruition and think it will have a positive impact on our\n",
      "city's economy and residents. Let's keep the momentum going!\n"
     ]
    }
   ],
   "source": [
    "# Collect context\n",
    "contexto = get_context(id, max_pais=3, token_threshold=200)\n",
    "\n",
    "comentario = get_post_text(id)\n",
    "\n",
    "text_input = contexto + \"\\n\\n*Comment to analyze:* \" + comentario\n",
    "\n",
    "print(text_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d5ad2250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Creativity']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].get(\"categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c1f897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”— A usar OpenAI (gpt-4o)...\n",
      "ðŸ“¤ Resposta do avaliador:\n",
      " ```json\n",
      "{\n",
      "  \"argument_extraction\": 4,\n",
      "  \"motivation_plausability\": 3,\n",
      "  \"category_score\": 50,\n",
      "  \"feedback\": \"The argument extraction is mostly coherent with the post, as the author does express appreciation for the design and architecture. However, the argument could be more comprehensive by including the author's positive view on the environmental impact and transparency aspects. The motivation of 'appreciation for modern design and architecture due to their aesthetic appeal' is plausible but somewhat limited, as the author also values the project's environmental considerations and community engagement. The Max-Neef category of 'Creativity' is partially correct, as it reflects the appreciation for design, but 'Understanding' could also be relevant due to the author's interest in the project's transparency and environmental impact. Including 'Participation' might also be appropriate given the author's appreciation for community engagement.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "resposta = generate_evaluation_response(\n",
    "    post_text=text_input,\n",
    "    argument=results[0].get(\"descriptions\"),\n",
    "    motivation=results[0].get(\"motivations\"),\n",
    "    categories=results[0].get(\"categories\")\n",
    ")\n",
    "\n",
    "print(\"ðŸ“¤ Evaluator's response:\\n\", resposta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a153e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported to avaliacoes\\resultados_gpt-4o_20250518.json\n"
     ]
    }
   ],
   "source": [
    "# Prepare filename with model name and current date\n",
    "model_name = OLLAMA_MODEL if not USE_OPENAI else OPENAI_MODEL\n",
    "date_str = datetime.now().strftime(\"%Y%m%d\")\n",
    "filename = f\"resultados_{model_name}_{date_str}.json\"\n",
    "\n",
    "# Parse resposta string to dict if needed\n",
    "if isinstance(resposta, str):\n",
    "    try:\n",
    "        resposta_dict = json.loads(resposta)\n",
    "    except json.JSONDecodeError:\n",
    "        resposta_dict = {\"resposta\": resposta}\n",
    "else:\n",
    "    resposta_dict = resposta\n",
    "\n",
    "# Add model name to the exported data\n",
    "export_data = {\n",
    "    \"model_name\": model_name,\n",
    "    \"result\": resposta_dict\n",
    "}\n",
    "\n",
    "# Ensure the \"avaliacoes\" folder exists\n",
    "os.makedirs(\"avaliacoes\", exist_ok=True)\n",
    "output_path = os.path.join(\"avaliacoes\", filename)\n",
    "\n",
    "# Export to JSON file\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(export_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Exported to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
